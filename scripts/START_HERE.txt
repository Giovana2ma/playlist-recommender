================================================================================
          CI/CD TESTING INFRASTRUCTURE FOR ARGOCD DEPLOYMENT
                     Playlist Recommender System
================================================================================

ğŸ“¦ WHAT YOU HAVE NOW

A complete testing infrastructure to measure your CI/CD pipeline performance:

âœ… Monitoring Scripts
   - test_cicd.py: Continuously monitors service, detects changes
   - analyze_results.py: Analyzes test results, calculates metrics
   - visualize_results.py: Creates graphs and charts
   - run_tests.sh: Interactive testing assistant

âœ… Documentation
   - README.md: Complete tool documentation
   - QUICK_START.md: Step-by-step testing guide
   - TESTING_SUMMARY.md: Overview and expected results
   - test_procedures.md: Detailed testing procedures
   - CHECKLIST.md: Progress tracking checklist

âœ… Report Template
   - report_template.tex: LaTeX template for your PDF report

âœ… Server Improvements
   - Health check endpoints added to api/server.py
   - /api/health: For Kubernetes readiness probes
   - /api/stats: Model statistics endpoint

================================================================================
ğŸš€ HOW TO GET STARTED
================================================================================

STEP 1: Read the Documentation (15 min)
   Start here â†’ scripts/TESTING_SUMMARY.md
   Then read â†’ scripts/QUICK_START.md

STEP 2: Install Dependencies (5 min)
   pip install requests matplotlib

STEP 3: Choose Your Path

   PATH A - Interactive (Recommended for Beginners)
   â”œâ”€ Run: ./scripts/run_tests.sh
   â””â”€ Follow the interactive menu

   PATH B - Manual (More Control)
   â”œâ”€ Follow: scripts/QUICK_START.md
   â”œâ”€ Track progress: scripts/CHECKLIST.md
   â””â”€ Run tests one by one

================================================================================
ğŸ“‹ THE THREE TESTS YOU NEED TO PERFORM
================================================================================

TEST 1: Replica Scaling
   â”œâ”€ Change: Increase/decrease replicas in deployment.yaml
   â”œâ”€ Duration: 15 minutes monitoring
   â”œâ”€ Expected: No downtime, 3-5 min deployment
   â””â”€ Output: test1_replicas.json

TEST 2: Code Update
   â”œâ”€ Change: Build new image version, update deployment
   â”œâ”€ Duration: 20 minutes monitoring
   â”œâ”€ Expected: 10-30s downtime, version change detected
   â””â”€ Output: test2_code.json

TEST 3: Dataset Update
   â”œâ”€ Change: Switch from ds1 to ds2 in ConfigMap
   â”œâ”€ Duration: 30 minutes monitoring
   â”œâ”€ Expected: ML job runs, model_date changes, 30-60s downtime
   â””â”€ Output: test3_dataset.json

================================================================================
â±ï¸  TIME ESTIMATE
================================================================================

Setup and Learning       30 minutes
Test 1 Execution        20 minutes (15 monitoring + 5 analysis)
Test 2 Execution        25 minutes (20 monitoring + 5 analysis)
Test 3 Execution        35 minutes (30 monitoring + 5 analysis)
Visualization           15 minutes
Report Writing          2-3 hours
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL                   4-5 hours

================================================================================
ğŸ“Š WHAT METRICS YOU'LL MEASURE
================================================================================

For each test:
   âœ“ Deployment time (end-to-end)
   âœ“ ArgoCD sync time
   âœ“ Service downtime duration
   âœ“ Success rate / Availability
   âœ“ Number of failed requests
   âœ“ Response time statistics
   âœ“ Change detection timing

================================================================================
ğŸ“ FILES YOU'LL GENERATE
================================================================================

Test Results:
   test1_replicas.json       - Test 1 detailed results
   test2_code.json          - Test 2 detailed results
   test3_dataset.json       - Test 3 detailed results

Analysis:
   comparison_report.txt    - Summary of all tests

Visualizations:
   comparison.png           - Bar chart comparing all tests
   test1_replicas_timeline.png
   test1_replicas_response.png
   test2_code_timeline.png
   test2_code_response.png
   test3_dataset_timeline.png
   test3_dataset_response.png

Final Report:
   report_template.pdf      - Your final submission

================================================================================
ğŸ’¡ QUICK COMMAND REFERENCE
================================================================================

# Setup
pip install requests matplotlib
kubectl port-forward svc/playlist-recommender-svc 50013:50013 -n giovanamachado

# Run a test
python3 scripts/test_cicd.py http://localhost:50013/api/recommend \
  -d 15 -o test1_replicas.json

# Analyze results
python3 scripts/analyze_results.py test1_replicas.json

# Visualize
python3 scripts/visualize_results.py test1_replicas.json --all

# Generate comparison
python3 scripts/visualize_results.py test*.json --comparison comparison.png

# Compile report
cd scripts
pdflatex report_template.tex
pdflatex report_template.tex

================================================================================
ğŸ¯ YOUR GOAL
================================================================================

Create a PDF report that includes:

   âœ… Introduction and methodology
   âœ… System architecture description
   âœ… Results for all three tests (tables + graphs)
   âœ… Comparative analysis
   âœ… Discussion of downtime causes and mitigation
   âœ… Recommendations for improvement
   âœ… Conclusions

================================================================================
ğŸ“– DOCUMENTATION GUIDE
================================================================================

FIRST TIME?
   1. Read: TESTING_SUMMARY.md
   2. Read: QUICK_START.md
   3. Use: CHECKLIST.md to track progress

NEED DETAILS?
   - Complete docs: README.md
   - Detailed procedures: test_procedures.md

NEED HELP?
   - Troubleshooting: README.md â†’ Troubleshooting section
   - Common issues: QUICK_START.md â†’ Common Issues section

================================================================================
âš ï¸  IMPORTANT TIPS
================================================================================

âœ“ Use multiple terminal windows (3 recommended)
   Terminal 1: Port forwarding
   Terminal 2: Monitoring script
   Terminal 3: Making changes / checking status

âœ“ Take screenshots of everything
   - Monitoring output
   - kubectl commands
   - ArgoCD UI
   - Pod status

âœ“ Document timestamps
   - When you pushed changes
   - When ArgoCD synced
   - When changes were detected

âœ“ Be patient
   - ArgoCD polls every 3 minutes
   - ML jobs can take 10-15 minutes
   - Don't interrupt monitoring scripts

âœ“ Run tests during low-traffic periods
   - Minimizes user impact
   - Clearer results

================================================================================
ğŸ”§ TROUBLESHOOTING
================================================================================

Port forward died?
   pkill -f port-forward
   kubectl port-forward svc/playlist-recommender-svc 50013:50013 -n giovanamachado

Service not responding?
   kubectl get pods -n giovanamachado
   kubectl logs -l app=giovanamachado-playlist-recommender -n giovanamachado

ArgoCD not syncing?
   kubectl get applications -n argocd
   # Wait up to 3 minutes for automatic sync

ML job not created?
   kubectl logs -l app=ml-config-watcher -n giovanamachado

================================================================================
âœ… QUICK START COMMAND
================================================================================

Want to start immediately? Run this:

   ./scripts/run_tests.sh

This interactive script will guide you through everything!

================================================================================
ğŸ“š NEXT STEPS
================================================================================

1. Read this file completely
2. Read TESTING_SUMMARY.md for overview
3. Read QUICK_START.md for detailed steps
4. Choose your testing path (interactive or manual)
5. Run your tests
6. Generate visualizations
7. Write your report
8. Submit to Sakai

================================================================================

Good luck with your CI/CD testing! ğŸš€

For questions or issues, refer to the troubleshooting sections in:
   - README.md
   - QUICK_START.md

================================================================================
