\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{subcaption}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

\title{CI/CD Infrastructure Testing Report\\
Playlist Recommender System with ArgoCD}
\author{Your Name\\
Student ID}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive evaluation of the Continuous Integration/Continuous Deployment (CI/CD) infrastructure for a Kubernetes-based playlist recommendation system using ArgoCD. Three distinct deployment scenarios were tested: (1) Kubernetes deployment configuration changes, (2) application code updates, and (3) machine learning dataset updates. For each scenario, we measured deployment times, service availability, and downtime periods by continuously monitoring the service endpoints. The results demonstrate the effectiveness of automated deployment pipelines and identify optimization opportunities for minimizing service disruption during updates.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Background}
The playlist recommender system is a microservices-based application deployed on Kubernetes. It consists of:
\begin{itemize}
    \item A Flask-based API server providing song recommendations
    \item A machine learning pipeline generating association rules from Spotify playlist data
    \item Persistent storage for trained models
    \item Automated deployment via ArgoCD
\end{itemize}

\subsection{Objectives}
This study aims to:
\begin{enumerate}
    \item Evaluate ArgoCD's automated redeployment capabilities
    \item Measure deployment times for different types of updates
    \item Quantify application downtime during deployments
    \item Identify best practices for minimizing service disruption
\end{enumerate}

\subsection{Testing Methodology}
Each test scenario followed this procedure:
\begin{enumerate}
    \item Start continuous monitoring of the service endpoint
    \item Trigger a specific type of change (deployment config, code, or dataset)
    \item Commit and push changes to the Git repository
    \item Monitor ArgoCD synchronization and deployment
    \item Collect metrics on response times, errors, and downtime
    \item Analyze results to determine deployment time and service impact
\end{enumerate}

\section{System Architecture}

\subsection{Application Components}
% Describe your architecture
The system consists of the following components:

\begin{itemize}
    \item \textbf{Frontend API}: Flask-based REST API serving recommendations
    \item \textbf{ML Pipeline}: Batch job generating association rules using Apriori algorithm
    \item \textbf{ConfigMap Watcher}: Monitors for dataset changes and triggers ML jobs
    \item \textbf{Persistent Volume}: Shared storage for model files
\end{itemize}

\subsection{CI/CD Pipeline}
% Describe your CI/CD setup
\begin{enumerate}
    \item Developer commits changes to Git repository
    \item ArgoCD detects repository changes (polling every 3 minutes)
    \item ArgoCD synchronizes Kubernetes manifests
    \item Kubernetes performs rolling updates for Deployments
    \item ConfigMap watcher triggers ML job creation when dataset changes
    \item Deployment automatically restarts to load new models
\end{enumerate}

% Add architecture diagram here
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{architecture_diagram.png}
%     \caption{System Architecture Diagram}
%     \label{fig:architecture}
% \end{figure}

\section{Test 1: Kubernetes Deployment Changes}

\subsection{Test Description}
This test evaluates ArgoCD's response to changes in Kubernetes deployment specifications, specifically scaling the number of replicas from 2 to 3.

\subsection{Test Execution}
\begin{lstlisting}[language=bash, caption=Test 1 Commands]
# Start monitoring
python3 scripts/test_cicd.py http://localhost:50013/api/recommend \
    -d 15 -o test1_replicas.json

# Update deployment
# Edit k8s/base/deployment.yaml: replicas: 2 -> 3
git add k8s/base/deployment.yaml
git commit -m "Scale to 3 replicas"
git push
\end{lstlisting}

\subsection{Results}
% Replace with your actual results
\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Test Duration & 15.0 minutes \\
Total Requests & 450 \\
Successful Requests & 450 (100\%) \\
Failed Requests & 0 (0\%) \\
Downtime & 0 seconds \\
ArgoCD Sync Time & 2.5 minutes \\
New Pod Ready Time & 45 seconds \\
Total Deployment Time & 3.2 minutes \\
\bottomrule
\end{tabular}
\caption{Test 1: Replica Scaling Results}
\label{tab:test1}
\end{table}

\subsection{Analysis}
% Add your analysis
\textbf{Observations:}
\begin{itemize}
    \item No downtime was observed during scaling operations
    \item Kubernetes rolling update strategy maintained service availability
    \item ArgoCD detected changes within expected polling interval
    \item New replicas became ready within 45 seconds
\end{itemize}

\textbf{Deployment Timeline:}
\begin{enumerate}
    \item t=0: Git push completed
    \item t=2.5min: ArgoCD detected change and started sync
    \item t=2.8min: New pod creation started
    \item t=3.2min: New pod ready and receiving traffic
\end{enumerate}

% Add timeline graph if available
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\textwidth]{test1_timeline.png}
%     \caption{Test 1: Request Success Rate Over Time}
%     \label{fig:test1}
% \end{figure}

\section{Test 2: Code Updates}

\subsection{Test Description}
This test evaluates the deployment process when updating the application code by building a new container image with version 0.2 and updating the Kubernetes deployment to use the new image.

\subsection{Test Execution}
\begin{lstlisting}[language=bash, caption=Test 2 Commands]
# Start monitoring
python3 scripts/test_cicd.py http://localhost:50013/api/recommend \
    -d 20 -o test2_code.json

# Build and push new image
cd api
docker build -t docker.io/giovana2ma/playlists-frontend:0.2 .
docker push docker.io/giovana2ma/playlists-frontend:0.2

# Update deployment
# Edit k8s/base/deployment.yaml: image tag 0.1 -> 0.2
# Edit k8s/base/configmap.yaml: API_VERSION 1.0.0 -> 1.0.1
git add k8s/base/deployment.yaml k8s/base/configmap.yaml
git commit -m "Update to version 0.2"
git push
\end{lstlisting}

\subsection{Results}
% Replace with your actual results
\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Test Duration & 20.0 minutes \\
Total Requests & 600 \\
Successful Requests & 594 (99\%) \\
Failed Requests & 6 (1\%) \\
Downtime & 12 seconds \\
Version Changes Detected & 1 \\
ArgoCD Sync Time & 2.8 minutes \\
Pod Termination \& Creation & 35 seconds \\
Total Deployment Time & 3.5 minutes \\
\bottomrule
\end{tabular}
\caption{Test 2: Code Update Results}
\label{tab:test2}
\end{table}

\subsection{Analysis}
% Add your analysis
\textbf{Observations:}
\begin{itemize}
    \item Brief downtime occurred during pod replacement
    \item Version change was detected in API responses
    \item Rolling update strategy minimized but did not eliminate downtime
    \item 6 requests failed during the transition period
\end{itemize}

\textbf{Downtime Analysis:}
The 12-second downtime occurred because:
\begin{itemize}
    \item Old pods were terminated before new pods were fully ready
    \item Container image pull time added to startup delay
    \item Service selector temporarily had no healthy backends
\end{itemize}

\textbf{Recommendations:}
\begin{itemize}
    \item Increase readiness probe initialDelaySeconds
    \item Pre-pull images to nodes
    \item Use PodDisruptionBudget to ensure minimum availability
    \item Consider blue-green deployment for zero-downtime updates
\end{itemize}

\section{Test 3: Training Dataset Update}

\subsection{Test Description}
This test evaluates the automated ML pipeline by switching the training dataset from ds1 to ds2, which should trigger:
\begin{enumerate}
    \item ConfigMap update detection
    \item ML job creation and execution
    \item Model file generation
    \item Deployment restart to load new model
\end{enumerate}

\subsection{Test Execution}
\begin{lstlisting}[language=bash, caption=Test 3 Commands]
# Start monitoring
python3 scripts/test_cicd.py http://localhost:50013/api/recommend \
    -d 30 -o test3_dataset.json

# Update ConfigMap
# Edit k8s/base/configmap.yaml:
#   DATASET_URL: .../2023_spotify_ds2.csv
#   DATASET_NAME: ds2
#   MODEL_FILENAME: rules_ds2.pkl
git add k8s/base/configmap.yaml
git commit -m "Switch to ds2 dataset"
git push
\end{lstlisting}

\subsection{Results}
% Replace with your actual results
\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Test Duration & 30.0 minutes \\
Total Requests & 900 \\
Successful Requests & 882 (98\%) \\
Failed Requests & 18 (2\%) \\
Downtime & 36 seconds \\
Model Changes Detected & 1 \\
ArgoCD Sync Time & 2.5 minutes \\
ML Job Execution Time & 8.5 minutes \\
Pod Restart Time & 30 seconds \\
Total Pipeline Time & 11.5 minutes \\
\bottomrule
\end{tabular}
\caption{Test 3: Dataset Update Results}
\label{tab:test3}
\end{table}

\subsection{Analysis}
% Add your analysis
\textbf{Observations:}
\begin{itemize}
    \item ConfigMap watcher detected change within 30 seconds
    \item ML job was automatically created and executed
    \item Model generation took 8.5 minutes for ds2
    \item Deployment restart caused brief downtime
    \item Model date in responses changed, confirming new model loaded
\end{itemize}

\textbf{Pipeline Timeline:}
\begin{enumerate}
    \item t=0: Git push completed
    \item t=2.5min: ArgoCD synced ConfigMap
    \item t=3.0min: Watcher detected change, created ML job
    \item t=11.5min: ML job completed, model file written
    \item t=11.5min: Deployment restart triggered
    \item t=12.0min: New pods ready with updated model
\end{enumerate}

\textbf{Downtime Analysis:}
The 36-second downtime occurred during:
\begin{itemize}
    \item Pod termination: 10 seconds
    \item Container startup: 15 seconds
    \item Model file loading: 11 seconds
\end{itemize}

\section{Comparative Analysis}

\subsection{Deployment Time Comparison}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Test & ArgoCD Sync & Deployment & Total Time \\
\midrule
Replica Scaling & 2.5 min & 0.7 min & 3.2 min \\
Code Update & 2.8 min & 0.7 min & 3.5 min \\
Dataset Update & 2.5 min & 9.0 min & 11.5 min \\
\bottomrule
\end{tabular}
\caption{Deployment Time Comparison}
\label{tab:comparison}
\end{table}

\subsection{Downtime Comparison}
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Test & Downtime (s) & Failed Requests & Availability \\
\midrule
Replica Scaling & 0 & 0 & 100\% \\
Code Update & 12 & 6 & 99.0\% \\
Dataset Update & 36 & 18 & 98.0\% \\
\bottomrule
\end{tabular}
\caption{Downtime and Availability Comparison}
\label{tab:downtime}
\end{table}

\subsection{Key Findings}

\textbf{1. Scaling Operations are Zero-Downtime}
\begin{itemize}
    \item Kubernetes rolling updates work effectively for scaling
    \item No service interruption when adding/removing replicas
    \item Horizontal scaling is the safest deployment operation
\end{itemize}

\textbf{2. Code Updates Cause Brief Downtime}
\begin{itemize}
    \item Pod replacement leads to 10-15 second downtime
    \item Image pull time affects deployment speed
    \item Rolling update strategy limits but doesn't eliminate downtime
\end{itemize}

\textbf{3. ML Pipeline Updates Have Longest Deployment Time}
\begin{itemize}
    \item Dataset processing dominates total deployment time
    \item Automated pipeline works correctly
    \item Model loading adds to pod startup time
\end{itemize}

\section{Discussion}

\subsection{CI/CD Pipeline Effectiveness}
The ArgoCD-based CI/CD pipeline successfully automates deployments for all three scenarios:
\begin{itemize}
    \item GitOps workflow is reliable and predictable
    \item Automated synchronization reduces manual intervention
    \item Declarative configuration simplifies rollbacks
    \item Integration with Kubernetes provides robust orchestration
\end{itemize}

\subsection{Downtime Causes and Mitigation}

\textbf{Primary Causes of Downtime:}
\begin{enumerate}
    \item Pod termination before replacement pods are ready
    \item Container image pull delays
    \item Application startup time (model loading)
    \item Health check initialization delays
\end{enumerate}

\textbf{Mitigation Strategies:}
\begin{enumerate}
    \item \textbf{Readiness Probes}: Ensure pods only receive traffic when ready
    \begin{lstlisting}
readinessProbe:
  httpGet:
    path: /api/health
    port: 50013
  initialDelaySeconds: 30
  periodSeconds: 5
    \end{lstlisting}
    
    \item \textbf{Pod Disruption Budgets}: Maintain minimum available replicas
    \begin{lstlisting}
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: playlist-recommender-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: giovanamachado-playlist-recommender
    \end{lstlisting}
    
    \item \textbf{Rolling Update Strategy}: Control update pace
    \begin{lstlisting}
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 0
    maxSurge: 1
    \end{lstlisting}
    
    \item \textbf{Image Pull Policy}: Pre-pull images to nodes
    \item \textbf{Blue-Green Deployments}: For critical zero-downtime updates
\end{enumerate}

\subsection{Trade-offs}

\textbf{Deployment Speed vs. Safety:}
\begin{itemize}
    \item Faster updates increase downtime risk
    \item Conservative rolling updates are slower but safer
    \item Trade-off depends on application criticality
\end{itemize}

\textbf{Automation vs. Control:}
\begin{itemize}
    \item Full automation reduces manual errors
    \item Manual approval gates increase deployment time
    \item Hybrid approach balances speed and control
\end{itemize}

\textbf{Resource Usage vs. Availability:}
\begin{itemize}
    \item More replicas improve availability during updates
    \item Higher resource costs
    \item Optimal replica count depends on traffic patterns
\end{itemize}

\section{Recommendations}

\subsection{Short-term Improvements}
\begin{enumerate}
    \item Implement health check endpoints in API
    \item Configure readiness probes with appropriate delays
    \item Create PodDisruptionBudget for minimum availability
    \item Optimize rolling update strategy parameters
    \item Pre-pull container images to worker nodes
\end{enumerate}

\subsection{Long-term Improvements}
\begin{enumerate}
    \item Implement blue-green deployment for code updates
    \item Use canary deployments for gradual rollouts
    \item Add monitoring and alerting for deployment issues
    \item Implement automated rollback on health check failures
    \item Consider service mesh for advanced traffic management
\end{enumerate}

\subsection{ML Pipeline Optimization}
\begin{enumerate}
    \item Cache frequently used datasets
    \item Parallelize model generation where possible
    \item Use incremental learning to reduce training time
    \item Separate model training from deployment pipeline
    \item Consider async model updates without pod restarts
\end{enumerate}

\section{Conclusions}

This study evaluated the CI/CD infrastructure for a Kubernetes-based playlist recommender system using ArgoCD. Through three distinct test scenarios, we measured deployment times, service availability, and downtime characteristics.

\textbf{Key Results:}
\begin{itemize}
    \item Replica scaling: 3.2 minutes, 0 seconds downtime (100\% availability)
    \item Code updates: 3.5 minutes, 12 seconds downtime (99\% availability)
    \item Dataset updates: 11.5 minutes, 36 seconds downtime (98\% availability)
\end{itemize}

\textbf{Main Conclusions:}
\begin{enumerate}
    \item ArgoCD effectively automates GitOps-based deployments
    \item Kubernetes rolling updates minimize but don't eliminate downtime
    \item ML pipeline automation successfully triggers on configuration changes
    \item Brief downtime is acceptable for non-critical applications
    \item Further optimization can achieve near-zero downtime if required
\end{enumerate}

The CI/CD pipeline demonstrates good performance for automated deployments while identifying specific areas for improvement to achieve higher availability during updates.

\section{Appendix}

\subsection{Test Environment}
\begin{itemize}
    \item Kubernetes Version: [Add your version]
    \item ArgoCD Version: [Add your version]
    \item Cluster: [Your cluster details]
    \item Node Count: [Your node count]
\end{itemize}

\subsection{Tools and Scripts}
All testing scripts are available in the project repository:
\begin{itemize}
    \item \texttt{scripts/test\_cicd.py}: Continuous monitoring tool
    \item \texttt{scripts/analyze\_results.py}: Results analysis tool
    \item \texttt{scripts/test\_procedures.md}: Detailed testing procedures
\end{itemize}

\subsection{Raw Test Data}
Complete test results are available in JSON format:
\begin{itemize}
    \item \texttt{test1\_replicas.json}
    \item \texttt{test2\_code.json}
    \item \texttt{test3\_dataset.json}
\end{itemize}

\end{document}
